{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "99db660a-119b-4720-b7db-7e3ebbb80d80",
    "_execution_state": "idle",
    "_uuid": "d390300d4c49a69634911ae2f56d770cac96c232"
   },
   "source": [
    "## Keypoints:\n",
    "\n",
    "* a collection of useful snippet of codes that you can use in other tasks\n",
    "* examples of how to monitor the performance of a ML working on imperfect data\n",
    "* examples of designing an ML model to classify using \"Decision Trees\" and \"Random Forests\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "99db660a-119b-4720-b7db-7e3ebbb80d80",
    "_execution_state": "idle",
    "_uuid": "d390300d4c49a69634911ae2f56d770cac96c232"
   },
   "outputs": [],
   "source": [
    "# import the packages we need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "29d2c2ce-cb68-40fc-928e-60af3b8c8958",
    "_uuid": "39f37ec345fdabbc16e109345a1ae5b591bbddc0"
   },
   "outputs": [],
   "source": [
    "# get titanic passenger list csv file as a DataFrame\n",
    "titanic_df = pd.read_csv(\"data/titanic_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b3d18fa2-bab7-4a8c-89c5-a7f57a7b808b",
    "_uuid": "07b7d5f4d5ac989164ddec4d7ca05cf29677d8db"
   },
   "outputs": [],
   "source": [
    "# preview the data\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a shortcut to get the null values\n",
    "titanic_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8342ac2c-a07c-495f-bec4-30acf480a6ab",
    "_uuid": "835f484bb1c50eb4c193604bb49ddbf4afcb2e6e"
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns; these columns won't be useful in analysis and prediction.\n",
    "# i.e. We are arguing that the passenger ID number, Name, Ticket code and \"Cabin\" have no predictive power of survival. That\n",
    "# may or may not be true; interesting to explore, especially \"Cabin\", but unfortunately there are many missing data.\n",
    "\n",
    "# axis=1 tells .drop to look at the columns\n",
    "\n",
    "titanic_df = titanic_df.drop(['PassengerId','Name','Ticket', 'Cabin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6b9f0b1b-cf91-400a-8cc4-180b502f4390",
    "_uuid": "ac3845694f67af7e85367422c759496d37b65244"
   },
   "outputs": [],
   "source": [
    "# How many passengers embarked where? S = Southampton, C=Cherbourg, Q=Queenstown\n",
    "titanic_df[\"Embarked\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6b9f0b1b-cf91-400a-8cc4-180b502f4390",
    "_uuid": "ac3845694f67af7e85367422c759496d37b65244"
   },
   "source": [
    "* the S (Southampton) is the most frequent departure harbour, so we will use that to fill any Null values (there are only 2). This is quite arbitrary, but shows AN approach to dealing with missing data. I'm sure you could think of others..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6b9f0b1b-cf91-400a-8cc4-180b502f4390",
    "_uuid": "ac3845694f67af7e85367422c759496d37b65244"
   },
   "outputs": [],
   "source": [
    "# in titanic_df, fill the two missing values with the most frequent value, which is \"S\".\n",
    "titanic_df[\"Embarked\"] = titanic_df[\"Embarked\"].fillna(\"S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check it's worked\n",
    "titanic_df[\"Embarked\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* We will turn the categorical data on embarkation (S, C, Q) into columns of 0s and 1s using the get_dummies method. \n",
    "\n",
    "* Assigning numerical values to non-numeric data is often/usually necessary. Sci-kit learn wants numeric values to things we want to use as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6b9f0b1b-cf91-400a-8cc4-180b502f4390",
    "_uuid": "ac3845694f67af7e85367422c759496d37b65244"
   },
   "outputs": [],
   "source": [
    "# make a new dummy dataframe with columns of 0s and 1s depending on embarkation port\n",
    "embark_dummies_titanic  = pd.get_dummies(titanic_df['Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6b9f0b1b-cf91-400a-8cc4-180b502f4390",
    "_uuid": "ac3845694f67af7e85367422c759496d37b65244",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show what this has done\n",
    "embark_dummies_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6b9f0b1b-cf91-400a-8cc4-180b502f4390",
    "_uuid": "ac3845694f67af7e85367422c759496d37b65244"
   },
   "outputs": [],
   "source": [
    "# this adds the three new columns in embark_dummies_titanic to the original dataframe\n",
    "titanic_df = titanic_df.join(embark_dummies_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show what we have so far\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6b9f0b1b-cf91-400a-8cc4-180b502f4390",
    "_uuid": "ac3845694f67af7e85367422c759496d37b65244"
   },
   "outputs": [],
   "source": [
    "# and because the idea was to replace the Embarked column, we now drop that from the dataframe\n",
    "titanic_df = titanic_df.drop(['Embarked'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to look at the ages and where the age is missing (177 values), replace it with an appropriate random age. Again, this is arbitrary - you can think of other methods..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of the ages with seaborn. This will ignore the missing values.\n",
    "sns.histplot(titanic_df[\"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "088600e9-ca97-4dc0-b776-86f8e840ef85",
    "_uuid": "0225a6a73f121a5244e812501e5fe66b8d8384e9"
   },
   "outputs": [],
   "source": [
    "# Some basic statistical methods in pandas on the \"Age\" column\n",
    "# get average, std, and number of NaN values in titanic_df[\"Age\"]\n",
    "average_age_titanic   = titanic_df[\"Age\"].mean()\n",
    "std_age_titanic       = titanic_df[\"Age\"].std()\n",
    "count_nan_age_titanic = titanic_df[\"Age\"].isnull().sum()\n",
    "print(\"ages: mean, stdev, number_nan\", average_age_titanic, std_age_titanic, count_nan_age_titanic)\n",
    "\n",
    "# Assign a random number from a normal distibution with the same mean and standard deviation.\n",
    "# pull random numbers from a normal distribution\n",
    "random_floats = np.random.normal(loc=average_age_titanic, scale=std_age_titanic, size=count_nan_age_titanic)\n",
    "# round them to 1 decimal place\n",
    "random_ages = np.round(random_floats, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should have an array of 177 random ages\n",
    "random_ages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "088600e9-ca97-4dc0-b776-86f8e840ef85",
    "_uuid": "0225a6a73f121a5244e812501e5fe66b8d8384e9"
   },
   "outputs": [],
   "source": [
    "# fill NaN values in Age column with random values generated\n",
    "titanic_df[\"Age\"][np.isnan(titanic_df[\"Age\"])] = random_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df[\"Age\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of the ages - should look slightly different to the previous example\n",
    "sns.histplot(titanic_df[\"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what have we got so far\n",
    "titanic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, for fun, create a couple of subsets for those that died or survived\n",
    "survived = titanic_df[ titanic_df[\"Survived\"] == 1 ]\n",
    "died = titanic_df[titanic_df[\"Survived\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use some pandas dataframe methods to plot semi-transparent age histograms for both subsets\n",
    "survived[\"Age\"].plot.hist(alpha=0.3,color='red',bins=50).set_xlabel(\"Age\")\n",
    "died[\"Age\"].plot.hist(alpha=0.3,color='blue',bins=50).set_xlabel(\"Age\")\n",
    "plt.legend(['Survived','Died'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more fun we'll examine whether there are correlations between survival or not with the continuously distributed\n",
    "# variables \"Age\" and \"Fare\" using a seaborn pairplot\n",
    "\n",
    "pairplot_df = titanic_df[[\"Survived\", \"Age\",\"Fare\"]]\n",
    "sns.pairplot(pairplot_df,hue='Survived',palette='Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cdfb5464-2f89-4dc4-872c-d429dcf4b8f2",
    "_uuid": "14e53e412fe95b255c4c66f07bacdcaa70ad7dd9"
   },
   "source": [
    "* Now we are going to go about turning other columns into numbers. First create a new column called Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cdfb5464-2f89-4dc4-872c-d429dcf4b8f2",
    "_uuid": "14e53e412fe95b255c4c66f07bacdcaa70ad7dd9"
   },
   "outputs": [],
   "source": [
    "# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.\n",
    "# So, maybe we could classify passengers as male, female or child\n",
    "# this is an example of writing a little function that can be used on a dataframe\n",
    "def get_person(passenger):\n",
    "    # passenger is going to contain both the age and sex of the passenger\n",
    "    age,sex = passenger\n",
    "    if age < 16:\n",
    "        return 'child'  \n",
    "    else:\n",
    "        return sex\n",
    "    \n",
    "titanic_df['Person'] = titanic_df[['Age','Sex']].apply(get_person,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first 10 rows to see what's been created\n",
    "titanic_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cdfb5464-2f89-4dc4-872c-d429dcf4b8f2",
    "_uuid": "14e53e412fe95b255c4c66f07bacdcaa70ad7dd9"
   },
   "source": [
    "* and now the dummies variable for the person categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cdfb5464-2f89-4dc4-872c-d429dcf4b8f2",
    "_uuid": "14e53e412fe95b255c4c66f07bacdcaa70ad7dd9"
   },
   "outputs": [],
   "source": [
    "# create dummy variables for Person column\n",
    "person_dummies_titanic  = pd.get_dummies(titanic_df['Person'])\n",
    "\n",
    "titanic_df = titanic_df.join(person_dummies_titanic)\n",
    "\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cdfb5464-2f89-4dc4-872c-d429dcf4b8f2",
    "_uuid": "14e53e412fe95b255c4c66f07bacdcaa70ad7dd9"
   },
   "outputs": [],
   "source": [
    "# No need to use Sex or Person columns once we have our numerical data\n",
    "titanic_df = titanic_df.drop(['Sex', 'Person'], axis=1)\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Decision Tree Classifier\n",
    "Having prepared a dataset and got rid of or replaced missing data, we move to the \"Machine Learning\" part.\n",
    "We're going to see how well we can predict the survival of passengers based on the data above. The \"label\" is the \"Survived\" value (1 = survived), the other columns are the \"features\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "20d906f4-4450-4037-b466-d3d31da10479",
    "_uuid": "01285817befba9347f29c4f5912b62f53367233a"
   },
   "outputs": [],
   "source": [
    "# define training and testing sets\n",
    "X = titanic_df.drop(\"Survived\",axis=1) # these are the features (in general, a matrix)\n",
    "y = titanic_df[\"Survived\"]             # these are the labels   (n general, a matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use a method from sklearn that will give us a random split between a training set and a test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30) # we choose 30%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build the model in scikit learn\n",
    "in this case, a decision tree classifier https://scikit-learn.org/stable/modules/tree.html#tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the model class from sklearn \n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "dtree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the data to the model - this IS the training step\n",
    "dtree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Evaluation \n",
    "\n",
    "\n",
    "* a prediction on a new data point is made by checking which region of the partition of the feature space the point lies in and then predicting the majority target (or the single target of pure leaves)\n",
    "* it is also possible to use decision tree for regression tasks. we still find the region where the new data point lies in but this time we calculate the mean target value of the training points in this leaf.\n",
    "\n",
    "* store the prediction in a variable called predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how we're doing by making predictions of the labels using the features in the test set and then comparing them with \n",
    "#the actual labels in the test set\n",
    "predictions = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usea couple of standard methods from the sklearn library\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print (classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to read this is:\n",
    "\n",
    "Precision: The ability of a classification model to identify only the relevant data points. Mathematically, precision is the number of true positives divided by the number of true positives plus the number of false positives.\n",
    "\n",
    "Recall: The ability of a model to find all the relevant cases within a data set. Mathematically, we define recall as the number of true positives divided by the number of true positives plus the number of false negatives.\n",
    "\n",
    "The f1-score is calculated from the \"confusion matrix\" - see below and https://builtin.com/data-science/precision-and-recall\n",
    "\n",
    "Support is how many of the sample were actually in each category.\n",
    "\n",
    "macro avg is an unweighted mean of the scores, while weighted avg weights by the numbers in each category (support)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (confusion_matrix(y_test,predictions))\n",
    "\n",
    "# The 2x2 confusion matrix here gives the number of true positives (class 0) in the top left,\n",
    "# the number of true negatives (class 1) in the bottom right and then bottom left is the\n",
    "# false negatives (1s that were classified as 0s) and top right is the false positives (0s that\n",
    "# were classified as 1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling complexity\n",
    "\n",
    "There is always a danger of underfitting or overfitting the model in any Machine Learning process. There are tools to examine whether that is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We see that the accuracy is very high in training : we let it run until it finds \"pure leaves\".\n",
    "* however, it performs much worse on the test data. This suggests overfitting.\n",
    "* We can try to avoid overfitting by restrict the depth of the decision tree. In scikit-learn we can appy 'pre-pruning' that will stop developing the tree before it perfectly fits to the training data.\n",
    "* We want to do so to avoid overfitting and create a model that is more robust to generalization\n",
    "\n",
    "\n",
    "### Create a second model\n",
    "\n",
    "* technique called Pruning\n",
    "The default is to keep making split decisions until we end up with as many leaves as training data points - almost perfect fitting. We can stop this by only allowing it to make a certain number of branching decisions before stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree2 = DecisionTreeClassifier(max_depth= 2)\n",
    "dtree2.fit(X_train, y_train)\n",
    "\n",
    "dtree2.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree2.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores above should be much more similar, suggesting that overfitting has not occurred. Of course we may also have lost some accuracy in our predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsDtree = dtree2.predict(X_test)\n",
    "print (classification_report(y_test,predictionsDtree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "\n",
    "* feature importance rates how important each feature is for the decision a tree makes\n",
    "* it is a number from 0 to 1. where 0 means not used at all (they all sum to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method the get the feature importance\n",
    "dtree.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree as tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the feature labels from the dataframe as the labels on a histogram plot\n",
    "titanic_df.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X.shape[1]\n",
    "plt.barh(range(n_features),dtree.feature_importances_)\n",
    "plt.yticks(np.arange(n_features),titanic_df.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree.plot_tree(dtree) # This would be very slow because the tree is very complex for model 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature importance for dtree2\n",
    "\n",
    "n_features = X.shape[1]\n",
    "plt.barh(range(n_features),dtree2.feature_importances_)\n",
    "plt.yticks(np.arange(n_features),titanic_df.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the limited depth of the tree. The X array refers to the columns\n",
    "# of features in the test set. It uses only the features in the plot above to make a categorisation.\n",
    "\n",
    "tree.plot_tree(dtree2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "\n",
    "* the algorithm is invariant to scaling of the data.\n",
    "* as each feature is processed separately, and the possible splits of the data don't depend on scaling, no preprocessing like normalisation or standardisation of features is needed for decision tree algorithms, so decision tree work well when you have features that are on completely different scales, or a mix of binary and continuous values. (but you might to scale the data for visualisation purposes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "* Random Forest is one of the most common ensemble methods, which consists of a collection of Decision Trees. \n",
    "* We repeatedly select data from the data set (with replacement) and build a Decision Tree with each new sample.\n",
    "* It is important to note that since we are sampling with replacement, many data points will be repeated and many wonâ€™t be included as well. \n",
    "* Random Forest is that each node of the Decision Tree is limited to only considering splits on random subsets of the features.\n",
    "\n",
    "#### How it works\n",
    "\n",
    "* In the case of classification with Random Forests, we use each tree in our forest to get a prediction, then the label with the most votes becomes the predicted class for that data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=10, max_depth=2) # 10 trees in the forest, to a maximum tree depth of 2\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test,rfc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classification_report(y_test,rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_features = X.shape[1]\n",
    "plt.barh(range(n_features),rfc.feature_importances_)\n",
    "plt.yticks(np.arange(n_features),titanic_df.columns[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOUR TURN\n",
    "\n",
    "\n",
    "* try to change the number of estimators of the Random Forest and see if that affects the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
